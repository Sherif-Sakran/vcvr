{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sherif\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script starts here\n",
      "Started recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 548/548 [00:35<00:00, 15.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done recording! - T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import sounddevice\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio\n",
    "from pydub import AudioSegment\n",
    "\n",
    "print(\"script starts here\")\n",
    "# speaker_name = input(\"Name: \")\n",
    "speaker_name = \"Silence\"\n",
    "# two recordings in Arabic and two in English. each recording is 60 seconds\n",
    "recording_number = \"1\"\n",
    "# recording_number = input(\"Recording id (e1, e2, a1, a2): \")\n",
    "# recording_length = int(input(\"Length in seocnds: \"))\n",
    "recording_length = 30\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "RECORD_SECONDS = int(recording_length) + 5.1\n",
    "WAVE_OUTPUT_FILENAME = speaker_name + \"_\" + recording_number +\".wav\"\n",
    "\n",
    "file_path = \"\" + WAVE_OUTPUT_FILENAME\n",
    "\n",
    "record = True\n",
    "vad = False\n",
    "\n",
    "if record:\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Started recording...\")\n",
    "    frames = []\n",
    "\n",
    "    for i in tqdm(range(0, int(RATE / CHUNK * RECORD_SECONDS))):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wf = wave.open(file_path, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    print(\"Done recording! - T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "output_file = file_path.split('.')[0]+ '.wav'\n",
    "\n",
    "if vad:\n",
    "    model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                                model='silero_vad',\n",
    "                                force_reload=True,\n",
    "                                onnx=False)\n",
    "\n",
    "    (get_speech_timestamps,\n",
    "    save_audio,\n",
    "    read_audio,\n",
    "    VADIterator,\n",
    "    collect_chunks) = utils\n",
    "\n",
    "    file = WAVE_OUTPUT_FILENAME\n",
    "    Audio(file)\n",
    "\n",
    "    output_file = file_path.split('.')[0]+'_only_speech'+ '.wav'\n",
    "    print(file)\n",
    "    wav = read_audio(file, sampling_rate=RATE)\n",
    "    # get speech timestamps from full audio file\n",
    "    speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=RATE)\n",
    "    # merge all speech chunks to one audio\n",
    "    if speech_timestamps:\n",
    "            save_audio(output_file,\n",
    "                    collect_chunks(speech_timestamps, wav), sampling_rate=RATE) \n",
    "            Audio(output_file)\n",
    "    else:\n",
    "            print(\"No activity detected\")\n",
    "    Audio(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import os\n",
    "\n",
    "def trim_wav(input_file, output_folder, segment_name, offset=0, length=3):\n",
    "    # Read the WAV file\n",
    "    sample_rate, audio_data = wavfile.read(input_file)\n",
    "\n",
    "    # Define the duration of each segment in samples (3 seconds)\n",
    "    segment_duration = length * sample_rate\n",
    "\n",
    "    # Calculate the number of segments\n",
    "    num_segments = len(audio_data) // segment_duration\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Trim the audio into segments\n",
    "    for i in range(num_segments):\n",
    "        start_sample = i * segment_duration\n",
    "        end_sample = (i + 1) * segment_duration\n",
    "        segment = audio_data[start_sample:end_sample]\n",
    "        # Save each segment as a separate WAV file\n",
    "        output_file = os.path.join(output_folder, f\"{segment_name}_{i+1+offset}.wav\")\n",
    "        wavfile.write(output_file, sample_rate, segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'Silence_1' already exists.\n"
     ]
    }
   ],
   "source": [
    "output_folder = output_file.split('.')[0]\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    print(f\"Directory '{output_folder}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Directory '{output_folder}' already exists.\")\n",
    "\n",
    "trim_wav(output_file, output_folder, output_folder, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Silence_1\\\\Silence_1_1.wav', 'Silence_1\\\\Silence_1_10.wav', 'Silence_1\\\\Silence_1_11.wav', 'Silence_1\\\\Silence_1_12.wav', 'Silence_1\\\\Silence_1_13.wav', 'Silence_1\\\\Silence_1_14.wav', 'Silence_1\\\\Silence_1_15.wav', 'Silence_1\\\\Silence_1_16.wav', 'Silence_1\\\\Silence_1_17.wav', 'Silence_1\\\\Silence_1_2.wav', 'Silence_1\\\\Silence_1_3.wav', 'Silence_1\\\\Silence_1_4.wav', 'Silence_1\\\\Silence_1_5.wav', 'Silence_1\\\\Silence_1_6.wav', 'Silence_1\\\\Silence_1_7.wav', 'Silence_1\\\\Silence_1_8.wav', 'Silence_1\\\\Silence_1_9.wav']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Replace 'directory_path' with the actual directory path you want to read files from\n",
    "directory_path = output_folder\n",
    "\n",
    "# Use glob to get a list of all WAV files in the directory recursively\n",
    "wav_files = glob.glob(directory_path + '/**/*.wav', recursive=True)\n",
    "\n",
    "# Print the list of WAV files\n",
    "print(wav_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in wav_files:\n",
    "    with wave.open(file_name, 'rb') as wav_file:\n",
    "        # Get the audio file properties\n",
    "        sample_width = wav_file.getsampwidth()\n",
    "        num_channels = wav_file.getnchannels()\n",
    "        sample_rate = wav_file.getframerate()\n",
    "        num_frames = wav_file.getnframes()\n",
    "\n",
    "        # Read the audio data\n",
    "        audio_data = wav_file.readframes(num_frames)\n",
    "\n",
    "    # Convert the audio data to AudioSegment object\n",
    "    audio = AudioSegment(\n",
    "        data=audio_data,\n",
    "        sample_width=sample_width,\n",
    "        frame_rate=sample_rate,\n",
    "        channels=num_channels\n",
    "    )\n",
    "\n",
    "    # Normalize the volume\n",
    "    normalized_audio = audio.normalize()\n",
    "\n",
    "    # Change the sampling rate to 1\n",
    "    normalized_audio = normalized_audio.set_frame_rate(16000)\n",
    "\n",
    "    # Combine all channels to one channel\n",
    "    normalized_audio = normalized_audio.set_channels(1)\n",
    "\n",
    "    # Export the normalized audio as WAV file\n",
    "    normalized_audio.export(file_name, format='wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silence_1/Silence_1_1.wav\n",
      "Silence_1/Silence_1_10.wav\n",
      "Silence_1/Silence_1_11.wav\n",
      "Silence_1/Silence_1_12.wav\n",
      "Silence_1/Silence_1_13.wav\n",
      "Silence_1/Silence_1_14.wav\n",
      "Silence_1/Silence_1_15.wav\n",
      "Silence_1/Silence_1_16.wav\n",
      "Silence_1/Silence_1_17.wav\n",
      "Silence_1/Silence_1_2.wav\n",
      "Silence_1/Silence_1_3.wav\n",
      "Silence_1/Silence_1_4.wav\n",
      "Silence_1/Silence_1_5.wav\n",
      "Silence_1/Silence_1_6.wav\n",
      "Silence_1/Silence_1_7.wav\n",
      "Silence_1/Silence_1_8.wav\n",
      "Silence_1/Silence_1_9.wav\n",
      "Silence_1.gmm\n",
      "+ modeling completed for speaker: Silence_1.gmm  with data point =  (3383, 22)\n",
      "Total Time taken:  0.73 seconds\n"
     ]
    }
   ],
   "source": [
    "import pickle as cPickle\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from FeatureExtraction import extract_features\n",
    "#from speakerfeatures import extract_features\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "time1 = time.time()\n",
    "training = True\n",
    "dest = \"models/\"\n",
    "\n",
    "training_size = 18\n",
    "\n",
    "directory = output_folder\n",
    "\n",
    "# 1 speaker\n",
    "# directory = directories[0]\n",
    "\n",
    "# List all files in the directory\n",
    "file_paths = os.listdir(directory)\n",
    "file_paths = sorted(file_paths)\n",
    "# print(file_paths)\n",
    "# Extracting features for each speaker\n",
    "features = np.asarray(())\n",
    "for path in file_paths:\n",
    "    path = directory + \"/\" + path    \n",
    "    path = path.strip()   \n",
    "    print (path)\n",
    "    \n",
    "    # read the audio\n",
    "    sr,audio = read(path)\n",
    "    \n",
    "    # extract 40 dimensional MFCC & delta MFCC features\n",
    "    vector   = extract_features(audio,sr)\n",
    "    \n",
    "    if features.size == 0:\n",
    "        features = vector\n",
    "    else:\n",
    "        features = np.vstack((features, vector))\n",
    "    # print(features.shape)\n",
    "    # when features of 5 files of speaker are concatenated, then do model training\n",
    "    # -> if count == 5: --> edited below\n",
    "\n",
    "    # speaker_dir = mfcc_dir + directory\n",
    "    # speaker_name = path.split(\"/\")[1].split(\".\")[0]\n",
    "\n",
    "    # if not os.path.exists(speaker_dir):\n",
    "    #     os.makedirs(speaker_dir)\n",
    "    # # Assuming 'features' is the 2D array containing the features\n",
    "    # csv_path = speaker_dir + \"/\" + speaker_name + \".csv\"\n",
    "    # np.savetxt(csv_path, vector, delimiter=\",\")\n",
    "    # print(\"Features saved to\", csv_path)\n",
    "    \n",
    "\n",
    "if training:            \n",
    "    gmm = GMM(n_components = 5, covariance_type='diag',n_init = 3)\n",
    "    gmm.fit(features)\n",
    "    # dumping the trained gaussian model\n",
    "    picklefile = directory + \".gmm\"\n",
    "    print(picklefile)\n",
    "    cPickle.dump(gmm,open(dest + picklefile,'wb'))\n",
    "    print ('+ modeling completed for speaker:',picklefile,\" with data point = \",features.shape)   \n",
    "    features = np.asarray(())\n",
    "print(\"Total Time taken: \", round(time.time() - time1, 2), \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
