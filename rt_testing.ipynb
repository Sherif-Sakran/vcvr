{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sherif\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import subprocess\n",
    "import sounddevice\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "import os\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "from FeatureExtraction import extract_features\n",
    "#from speakerfeatures import extract_features\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_Silence_1\n",
      "2_Omar_a1\n",
      "3_Silence_1\n",
      "4_Silence_1\n",
      "5_Silence_1\n",
      "6_Sherif_a1\n",
      "7_Sherif_e1\n",
      "8_Silence_1\n",
      "9_Silence_1\n",
      "10_Sherif_a1\n",
      "11_Silence_1\n",
      "12_Silence_1\n",
      "13_Sherif_a1\n",
      "14_Silence_1\n",
      "15_Silence_1\n",
      "16_Silence_1\n",
      "17_Youssef_e1\n",
      "18_Silence_1\n",
      "19_Sherif_e1\n",
      "20_Silence_1\n",
      "21_Sherif_e1\n",
      "22_Sherif_e1\n",
      "23_Silence_1\n",
      "24_Silence_1\n",
      "25_Sherif_a1\n",
      "26_Silence_1\n",
      "27_Silence_1\n",
      "28_Silence_1\n",
      "29_Sherif_a1\n",
      "30_Reem_a1\n",
      "31_Renad_a1\n",
      "32_Sherif_e1\n",
      "33_Silence_1\n",
      "34_Renad_e1\n",
      "35_Sherif_e1\n",
      "36_Silence_1\n",
      "37_Silence_1\n",
      "38_Sherif_a1\n",
      "39_Silence_1\n",
      "40_Silence_1\n",
      "41_Omar_a1\n",
      "42_Reem_e1\n",
      "43_Omar_a1\n",
      "44_Sherif_a1\n",
      "45_Reem_e1\n",
      "46_Omar_a1\n",
      "47_Sherif_e1\n",
      "48_Abdelrahman Said_e1\n",
      "49_Sherif_a1\n",
      "50_Youssef_e1\n",
      "51_Youssef_e1\n",
      "52_Youssef_e1\n",
      "53_Youssef_e1\n",
      "54_Omar_e1\n",
      "55_Sherif_a1\n",
      "56_Youssef_a1\n",
      "57_Silence_1\n",
      "58_Omar_a1\n",
      "59_Abdelrahman Said_e1\n",
      "60_Abdelrahman Said_e1\n",
      "61_Silence_1\n",
      "62_Sherif_e1\n",
      "63_Sherif_e1\n",
      "64_Sherif_e1\n",
      "65_Sherif_e1\n",
      "66_Renad_e1\n",
      "67_Reem_e1\n",
      "68_Sherif_a1\n",
      "69_Silence_1\n",
      "70_Silence_1\n",
      "71_Silence_1\n",
      "72_Sherif_e1\n",
      "73_Silence_1\n",
      "74_Silence_1\n",
      "75_Sherif_a1\n",
      "76_Omar_a1\n",
      "77_Silence_1\n",
      "78_Silence_1\n",
      "79_Silence_1\n",
      "80_Silence_1\n",
      "81_Sherif_a1\n",
      "82_Silence_1\n",
      "83_Silence_1\n",
      "84_Silence_1\n",
      "85_Sherif_e1\n",
      "86_Omar_a1\n",
      "87_Omar_a1\n",
      "88_Sherif_a1\n",
      "89_Silence_1\n",
      "90_Silence_1\n",
      "91_Silence_1\n",
      "92_Silence_1\n",
      "93_Silence_1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m data_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(RATE \u001b[38;5;241m/\u001b[39m CHUNK \u001b[38;5;241m*\u001b[39m record_len)):\n\u001b[1;32m---> 37\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m     39\u001b[0m     data_bytes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "same_output_file = False\n",
    "\n",
    "\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "RECORD_SECONDS = 1.1\n",
    "record_len = RECORD_SECONDS\n",
    "wav_path = \"output.wav\"\n",
    "record = True\n",
    "vad = True\n",
    "normalize = True\n",
    "recognize = True\n",
    "\n",
    "condition = True\n",
    "counter = 0\n",
    "frames = []\n",
    "while condition and counter < 700 :\n",
    "    # condition = False\n",
    "    counter += 1\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "    # print(\"Started recording...\", end=\"\")\n",
    "\n",
    "    if record:\n",
    "        # if frames:\n",
    "            # frames = frames[len(frames)//2:]\n",
    "            # record_len = RECORD_SECONDS - 1\n",
    "        frames = []\n",
    "        data_bytes = b''\n",
    "        for i in range(0, int(RATE / CHUNK * record_len)):\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)\n",
    "            data_bytes += data\n",
    "\n",
    "\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "        # print(type(data_bytes))\n",
    "        # sys.exit()\n",
    "        if same_output_file:\n",
    "            ext = \"\"\n",
    "        else: \n",
    "            ext = \"./recordings_dump/\" + str(counter)\n",
    "        wf = wave.open(ext+wav_path, 'wb')\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "        wf.close()\n",
    "        # print(\"Done recording! - F\")\n",
    "        # print(len(frames))\n",
    "\n",
    "    # if vad:\n",
    "    #     pass\n",
    "\n",
    "    if normalize:\n",
    "        with wave.open(ext+wav_path, 'rb') as wav_file:\n",
    "            # Get the audio file properties\n",
    "            sample_width = wav_file.getsampwidth()\n",
    "            num_channels = wav_file.getnchannels()\n",
    "            sample_rate = wav_file.getframerate()\n",
    "            num_frames = wav_file.getnframes()\n",
    "\n",
    "            # Read the audio data\n",
    "            audio_data = wav_file.readframes(num_frames)\n",
    "\n",
    "\n",
    "        # Convert the audio data to AudioSegment object\n",
    "        audio = AudioSegment(\n",
    "            data=audio_data,\n",
    "            sample_width=sample_width,\n",
    "            frame_rate=sample_rate,\n",
    "            channels=num_channels\n",
    "        )\n",
    "\n",
    "        # Normalize the volume\n",
    "        normalized_audio = audio.normalize()\n",
    "\n",
    "        # Change the sampling rate to 1\n",
    "        normalized_audio = normalized_audio.set_frame_rate(16000)\n",
    "\n",
    "        # Combine all channels to one channel\n",
    "        normalized_audio = normalized_audio.set_channels(1)\n",
    "\n",
    "        # Export the normalized audio as WAV file\n",
    "        normalized_audio.export(ext+wav_path.split(\".\")[0]+\"_normalized.wav\", format='wav')\n",
    "        path = ext+wav_path.split(\".\")[0]+\"_normalized.wav\"\n",
    "\n",
    "\n",
    "    if recognize:\n",
    "        #path where training speakers will be saved\n",
    "        modelpath = \"models/\"\n",
    "\n",
    "        gmm_files = [os.path.join(modelpath,fname) for fname in \n",
    "                    os.listdir(modelpath) if fname.endswith('.gmm')]\n",
    "\n",
    "        #Load the Gaussian Models\n",
    "        models    = [cPickle.load(open(fname,'rb')) for fname in gmm_files]\n",
    "        # print(\"Loaded Models: \", len(models))\n",
    "        speakers   = [fname.split(\"/\")[-1].split(\".gmm\")[0] for fname \n",
    "                    in gmm_files]\n",
    "\n",
    "        # path = wav_path.split(\".\")[0]+\"_normalized.wav\"\n",
    "        path = ext+wav_path\n",
    "        # print(\"Testing Audio : \", path)\n",
    "        time1 = time.time()   \n",
    "        sr,audio = read(path)\n",
    "        vector   = extract_features(audio,sr)\n",
    "        log_likelihood = np.zeros(len(models)) \n",
    "        for i in range(len(models)):\n",
    "            gmm    = models[i]  #checking with each model one by one\n",
    "            scores = np.array(gmm.score(vector))\n",
    "            log_likelihood[i] = scores.sum()\n",
    "        winner=np.argmax(log_likelihood)\n",
    "        winner_score = log_likelihood[winner]\n",
    "        predicted_speaker = speakers[winner]\n",
    "        sample_time = time.time() - time1\n",
    "        # print(f\"{predicted_speaker}\", log_likelihood)\n",
    "        print(f\"{counter}_{predicted_speaker}\")\n",
    "        # print(\"Time taken in ms: \", sample_time*1000)\n",
    "    # time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started recording...Done recording! - F\n",
      "Reem_a1 [-32.76447426 -32.30079641 -31.10354531 -31.91071523 -31.79944575\n",
      " -32.35427014]\n",
      "Started recording...Done recording! - F\n",
      "Reem_a1 [-32.56040529 -31.92070615 -31.52778457 -32.07814506 -31.91837925\n",
      " -32.29736347]\n",
      "Started recording...Done recording! - F\n",
      "Reem_a1 [-32.7011745  -32.31892179 -31.67203436 -32.12703129 -32.38901516\n",
      " -32.54915354]\n",
      "Started recording...Done recording! - F\n",
      "Reem_a1 [-32.59340194 -32.03538906 -31.70798461 -31.93584483 -32.28995086\n",
      " -32.17485973]\n",
      "Started recording...Done recording! - F\n",
      "Sherif_a1 [-31.51790128 -32.02687426 -31.42536591 -31.43124396 -31.09650497\n",
      " -31.38635346]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source   = \"2-wav_testing/\"   \n",
    "\n",
    "same_output_file = False\n",
    "\n",
    "\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "RECORD_SECONDS = 3.1\n",
    "record_len = RECORD_SECONDS\n",
    "wav_path = \"output.wav\"\n",
    "record = True\n",
    "vad = True\n",
    "normalize = True\n",
    "recognize = True\n",
    "\n",
    "condition = True\n",
    "counter = 5\n",
    "frames = []\n",
    "while condition and counter < 10 :\n",
    "    # condition = False\n",
    "    counter += 1\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "    print(\"Started recording...\", end=\"\")\n",
    "\n",
    "    if record:\n",
    "        if frames:\n",
    "            frames = frames[len(frames)//3:]\n",
    "            record_len = RECORD_SECONDS - 2\n",
    "        data_bytes = b''\n",
    "        for i in range(0, int(RATE / CHUNK * record_len)):\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)\n",
    "            data_bytes += data\n",
    "\n",
    "\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "        # print(type(data_bytes))\n",
    "        # sys.exit()\n",
    "        if same_output_file:\n",
    "            ext = \"\"\n",
    "        else: \n",
    "            ext = \"./recordings_dump/\" + str(counter)\n",
    "        wf = wave.open(ext+wav_path, 'wb')\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "        wf.close()\n",
    "        print(\"Done recording! - F\")\n",
    "        # print(len(frames))\n",
    "\n",
    "    # if vad:\n",
    "    #     pass\n",
    "\n",
    "    if normalize:\n",
    "        with wave.open(ext+wav_path, 'rb') as wav_file:\n",
    "            # Get the audio file properties\n",
    "            sample_width = wav_file.getsampwidth()\n",
    "            num_channels = wav_file.getnchannels()\n",
    "            sample_rate = wav_file.getframerate()\n",
    "            num_frames = wav_file.getnframes()\n",
    "\n",
    "            # Read the audio data\n",
    "            audio_data = wav_file.readframes(num_frames)\n",
    "\n",
    "\n",
    "        # Convert the audio data to AudioSegment object\n",
    "        audio = AudioSegment(\n",
    "            data=audio_data,\n",
    "            sample_width=sample_width,\n",
    "            frame_rate=sample_rate,\n",
    "            channels=num_channels\n",
    "        )\n",
    "\n",
    "        # Normalize the volume\n",
    "        normalized_audio = audio.normalize()\n",
    "\n",
    "        # Change the sampling rate to 1\n",
    "        normalized_audio = normalized_audio.set_frame_rate(16000)\n",
    "\n",
    "        # Combine all channels to one channel\n",
    "        normalized_audio = normalized_audio.set_channels(1)\n",
    "\n",
    "        # Export the normalized audio as WAV file\n",
    "        normalized_audio.export(ext+wav_path.split(\".\")[0]+\"_normalized.wav\", format='wav')\n",
    "        path = ext+wav_path.split(\".\")[0]+\"_normalized.wav\"\n",
    "\n",
    "\n",
    "    if recognize:\n",
    "        #path where training speakers will be saved\n",
    "        modelpath = \"models/\"\n",
    "\n",
    "        gmm_files = [os.path.join(modelpath,fname) for fname in \n",
    "                    os.listdir(modelpath) if fname.endswith('.gmm')]\n",
    "\n",
    "        #Load the Gaussian Models\n",
    "        models    = [cPickle.load(open(fname,'rb')) for fname in gmm_files]\n",
    "        # print(\"Loaded Models: \", len(models))\n",
    "        speakers   = [fname.split(\"/\")[-1].split(\".gmm\")[0] for fname \n",
    "                    in gmm_files]\n",
    "\n",
    "        # path = wav_path.split(\".\")[0]+\"_normalized.wav\"\n",
    "        path = ext+wav_path\n",
    "        # print(\"Testing Audio : \", path)\n",
    "        time1 = time.time()   \n",
    "        sr,audio = read(path)\n",
    "        vector   = extract_features(audio,sr)\n",
    "        log_likelihood = np.zeros(len(models)) \n",
    "        for i in range(len(models)):\n",
    "            gmm    = models[i]  #checking with each model one by one\n",
    "            scores = np.array(gmm.score(vector))\n",
    "            log_likelihood[i] = scores.sum()\n",
    "        winner=np.argmax(log_likelihood)\n",
    "        winner_score = log_likelihood[winner]\n",
    "        predicted_speaker = speakers[winner]\n",
    "        sample_time = time.time() - time1\n",
    "        print(f\"{predicted_speaker}\", log_likelihood)\n",
    "        # print(\"Time taken in ms: \", sample_time*1000)\n",
    "    # time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "silence:\n",
    "Sherif_a1 [-32.27361522 -30.88630014 -31.88865575 -33.01585415 -27.92731392\n",
    " -30.4037003 ]\n",
    "Started recording...Done recording! - F\n",
    "Sherif_a1 [-32.27361522 -30.88630014 -31.88865575 -33.01585415 -27.92731392\n",
    " -30.4037003 ]\n",
    "Started recording...Done recording! - F\n",
    "Sherif_a1 [-32.27361522 -30.88630014 -31.88865575 -33.01585415 -27.92731392\n",
    " -30.4037003 ]\n",
    "Started recording...Done recording! - F\n",
    "Sherif_a1 [-32.27361522 -30.88630014 -31.88865575 -33.01585415 -27.92731392\n",
    " -30.4037003 ]\n",
    "Started recording...Done recording! - F\n",
    "Sherif_a1 [-32.27361522 -30.88630014 -31.88865575 -33.01585415 -27.92731392\n",
    " -30.4037003 ]\n",
    "Started recording...Done recording! - F\n",
    "Sherif_a1 [-32.27361522 -30.88630014 -31.88865575 -33.01585415 -27.92731392\n",
    " -30.4037003 ]\n",
    "\n",
    " speech:\n",
    " Sherif_a1 [-32.27361522 -30.88630014 -31.88865575 -33.01585415 -27.92731392\n",
    " -30.4037003 ]\n",
    "Started recording...Done recording! - F\n",
    "Sherif_a1 [-32.27361522 -30.88630014 -31.88865575 -33.01585415 -27.92731392\n",
    " -30.4037003 ]\n",
    "Started recording...Done recording! - F\n",
    "Sherif_a1 [-32.27361522 -30.88630014 -31.88865575 -33.01585415 -27.92731392\n",
    " -30.4037003 ]\n",
    "Started recording...Done recording! - F\n",
    "Sherif_a1 [-32.27361522 -30.88630014 -31.88865575 -33.01585415 -27.92731392\n",
    " -30.4037003 ]\n",
    "Started recording...Done recording! - F\n",
    "Sherif_a1 [-32.27361522 -30.88630014 -31.88865575 -33.01585415 -27.92731392\n",
    " -30.4037003 ]\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
