{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as cPickle\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from FeatureExtraction import extract_features\n",
    "#from speakerfeatures import extract_features\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sherif', 'Youssef', 'Abdelrahman', 'Omar', 'Reem', 'Renad']\n",
      "Sherif/S_1.wav\n",
      "(299, 22)\n",
      "Sherif/S_10.wav\n",
      "(598, 22)\n",
      "Sherif/S_11.wav\n",
      "(897, 22)\n",
      "Sherif/S_12.wav\n",
      "(1196, 22)\n",
      "Sherif/S_13.wav\n",
      "(1495, 22)\n",
      "Sherif/S_14.wav\n",
      "(1794, 22)\n",
      "Sherif/S_15.wav\n",
      "(2093, 22)\n",
      "Sherif/S_16.wav\n",
      "(2392, 22)\n",
      "Sherif.gmm\n",
      "+ modeling completed for speaker: Sherif.gmm  with data point =  (2392, 22)\n",
      "Youssef/Y_1.wav\n",
      "(299, 22)\n",
      "Youssef/Y_10.wav\n",
      "(598, 22)\n",
      "Youssef/Y_11.wav\n",
      "(897, 22)\n",
      "Youssef/Y_12.wav\n",
      "(1196, 22)\n",
      "Youssef/Y_13.wav\n",
      "(1495, 22)\n",
      "Youssef/Y_14.wav\n",
      "(1794, 22)\n",
      "Youssef/Y_15.wav\n",
      "(2093, 22)\n",
      "Youssef/Y_16.wav\n",
      "(2392, 22)\n",
      "Youssef.gmm\n",
      "+ modeling completed for speaker: Youssef.gmm  with data point =  (2392, 22)\n",
      "Abdelrahman/A_1.wav\n",
      "(299, 22)\n",
      "Abdelrahman/A_10.wav\n",
      "(598, 22)\n",
      "Abdelrahman/A_11.wav\n",
      "(897, 22)\n",
      "Abdelrahman/A_12.wav\n",
      "(1196, 22)\n",
      "Abdelrahman/A_13.wav\n",
      "(1495, 22)\n",
      "Abdelrahman/A_14.wav\n",
      "(1794, 22)\n",
      "Abdelrahman/A_15.wav\n",
      "(2093, 22)\n",
      "Abdelrahman/A_16.wav\n",
      "(2392, 22)\n",
      "Abdelrahman.gmm\n",
      "+ modeling completed for speaker: Abdelrahman.gmm  with data point =  (2392, 22)\n",
      "Omar/O_1.wav\n",
      "(299, 22)\n",
      "Omar/O_10.wav\n",
      "(598, 22)\n",
      "Omar/O_11.wav\n",
      "(897, 22)\n",
      "Omar/O_12.wav\n",
      "(1196, 22)\n",
      "Omar/O_13.wav\n",
      "(1495, 22)\n",
      "Omar/O_14.wav\n",
      "(1794, 22)\n",
      "Omar/O_15.wav\n",
      "(2093, 22)\n",
      "Omar/O_16.wav\n",
      "(2392, 22)\n",
      "Omar.gmm\n",
      "+ modeling completed for speaker: Omar.gmm  with data point =  (2392, 22)\n",
      "Reem/Reem_1.wav\n",
      "(299, 22)\n",
      "Reem/Reem_10.wav\n",
      "(598, 22)\n",
      "Reem/Reem_11.wav\n",
      "(897, 22)\n",
      "Reem/Reem_12.wav\n",
      "(1196, 22)\n",
      "Reem/Reem_13.wav\n",
      "(1495, 22)\n",
      "Reem/Reem_14.wav\n",
      "(1794, 22)\n",
      "Reem/Reem_15.wav\n",
      "(2093, 22)\n",
      "Reem/Reem_16.wav\n",
      "(2392, 22)\n",
      "Reem.gmm\n",
      "+ modeling completed for speaker: Reem.gmm  with data point =  (2392, 22)\n",
      "Renad/Renad_1.wav\n",
      "(299, 22)\n",
      "Renad/Renad_10.wav\n",
      "(598, 22)\n",
      "Renad/Renad_11.wav\n",
      "(897, 22)\n",
      "Renad/Renad_12.wav\n",
      "(1196, 22)\n",
      "Renad/Renad_13.wav\n",
      "(1495, 22)\n",
      "Renad/Renad_14.wav\n",
      "(1794, 22)\n",
      "Renad/Renad_15.wav\n",
      "(2093, 22)\n",
      "Renad/Renad_16.wav\n",
      "(2392, 22)\n",
      "Renad.gmm\n",
      "+ modeling completed for speaker: Renad.gmm  with data point =  (2392, 22)\n",
      "Total Time taken:  3.04 seconds\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "training = True\n",
    "dest = \"2-gmm_models_scale_test_dataset/\"\n",
    "\n",
    "training_size = 8\n",
    "\n",
    "test = \"normalized\"\n",
    "\n",
    "subset = \"training\"\n",
    "audios = f\"../../dataset/{test}_dataset/{subset}/\"\n",
    "\n",
    "directories = os.listdir(audios)\n",
    "print(directories)\n",
    "\n",
    "\n",
    "\n",
    "for directory in directories:\n",
    "    # 1 speaker\n",
    "    # directory = directories[0]\n",
    "\n",
    "    # List all files in the directory\n",
    "    file_paths = os.listdir(audios + directory)\n",
    "    file_paths = sorted(file_paths)\n",
    "    file_paths = file_paths[:training_size]\n",
    "    # print(file_paths)\n",
    "    # Extracting features for each speaker\n",
    "    features = np.asarray(())\n",
    "    for path in file_paths:\n",
    "        path = directory + \"/\" + path    \n",
    "        path = path.strip()   \n",
    "        print (path)\n",
    "        \n",
    "        # read the audio\n",
    "        sr,audio = read(audios+path)\n",
    "        \n",
    "        # extract 40 dimensional MFCC & delta MFCC features\n",
    "        vector   = extract_features(audio,sr)\n",
    "        \n",
    "        if features.size == 0:\n",
    "            features = vector\n",
    "        else:\n",
    "            features = np.vstack((features, vector))\n",
    "        print(features.shape)\n",
    "        # when features of 5 files of speaker are concatenated, then do model training\n",
    "        # -> if count == 5: --> edited below\n",
    "\n",
    "        # speaker_dir = mfcc_dir + directory\n",
    "        # speaker_name = path.split(\"/\")[1].split(\".\")[0]\n",
    "\n",
    "        # if not os.path.exists(speaker_dir):\n",
    "        #     os.makedirs(speaker_dir)\n",
    "        # # Assuming 'features' is the 2D array containing the features\n",
    "        # csv_path = speaker_dir + \"/\" + speaker_name + \".csv\"\n",
    "        # np.savetxt(csv_path, vector, delimiter=\",\")\n",
    "        # print(\"Features saved to\", csv_path)\n",
    "        \n",
    "\n",
    "    if training and subset == \"training\":            \n",
    "        gmm = GMM(n_components = 5, covariance_type='diag',n_init = 3)\n",
    "        gmm.fit(features)\n",
    "        # dumping the trained gaussian model\n",
    "        picklefile = directory + \".gmm\"\n",
    "        print(picklefile)\n",
    "        cPickle.dump(gmm,open(dest + picklefile,'wb'))\n",
    "        print ('+ modeling completed for speaker:',picklefile,\" with data point = \",features.shape)   \n",
    "        features = np.asarray(())\n",
    "print(\"Total Time taken: \", round(time.time() - time1, 2), \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Models:  6\n",
      "Testing the model with all the samples...\n",
      "Renad:Reem, Renad:Reem, 2 120.0\n",
      "Accuracy:  0.9833\n",
      "Average time taken per sample in ms 12.82\n",
      "Preprocessing MisPredictions:  0\n",
      "2-gmm_models_scale_test_dataset/\n",
      "Speaker Identified Successfully\n"
     ]
    }
   ],
   "source": [
    "test = \"normalized\"\n",
    "subset = \"testing_same\"\n",
    "\n",
    "# test = \"normalized\"\n",
    "# subset = \"testing_different\"\n",
    "\n",
    "source = f\"../../dataset/{test}_dataset/{subset}/\"\n",
    "\n",
    "#path where training speakers will be saved\n",
    "modelpath = dest\n",
    "\n",
    "gmm_files = [os.path.join(modelpath,fname) for fname in \n",
    "              os.listdir(modelpath) if fname.endswith('.gmm')]\n",
    "\n",
    "#Load the Gaussian Models\n",
    "models    = [cPickle.load(open(fname,'rb')) for fname in gmm_files]\n",
    "print(\"Loaded Models: \", len(models))\n",
    "speakers   = [fname.split(\"/\")[-1].split(\".gmm\")[0] for fname \n",
    "              in gmm_files]\n",
    "\n",
    "error = 0\n",
    "total_samples = 0.0\n",
    "\n",
    "external_class_true_count = 0\n",
    "# print(\"Press '1' for checking a single Audio or Press '0' for testing a complete set of audio with Accuracy?\")\n",
    "# take=int(input().strip())\n",
    "\n",
    "predictions = []\n",
    "gender_predictions = []\n",
    "gender_gt = []\n",
    "y_true = []\n",
    "preprocessingMisPredictions = 0\n",
    "print(\"Testing the model with all the samples...\")\n",
    "false_predictions = []\n",
    "test_file = \"Testing_audio_Path.txt\"       \n",
    "# file_paths = open(test_file,'r')\n",
    "file_paths = glob.glob(f\"{source}/**/*.wav\", recursive=True)\n",
    "\n",
    "# print(file_paths)\n",
    "\n",
    "# Read the test directory and get the list of test audio files \n",
    "avg_time = 0\n",
    "total_time = 0\n",
    "for path in file_paths:\n",
    "    time1 = time.time()   \n",
    "    total_samples+= 1.0\n",
    "    path=path.strip()\n",
    "    # print(\"Testing Audio : \", path)\n",
    "    sr,audio = read(path)\n",
    "    vector   = extract_features(audio,sr)\n",
    "    # print(vector.shape)\n",
    "    log_likelihood = np.zeros(len(models)) \n",
    "    for i in range(len(models)):\n",
    "        gmm    = models[i]  #checking with each model one by one\n",
    "        scores = np.array(gmm.score(vector))\n",
    "        log_likelihood[i] = scores.sum()\n",
    "    winner=np.argmax(log_likelihood)\n",
    "    winner_score = log_likelihood[winner]\n",
    "    predicted_speaker = speakers[winner]\n",
    "    speaker_label = path.split(\"/\")[-2]\n",
    "    # print(\"OK score: \", winner_score)\n",
    "    predictions.append(predicted_speaker)\n",
    "    y_true.append(speaker_label)\n",
    "\n",
    "    if predicted_speaker != speaker_label:\n",
    "        print(f\"{speaker_label}:{predicted_speaker}, \", end=\"\")\n",
    "        error += 1\n",
    "        false_predictions.append(f\"{speaker_label}:{predicted_speaker} ({path.split('/')[-1]})\")\n",
    "    # time.sleep(1.0)\n",
    "    sample_time = time.time() - time1\n",
    "    total_time += sample_time\n",
    "print (error, total_samples)\n",
    "accuracy = ((total_samples - error) / total_samples)\n",
    "\n",
    "print (\"Accuracy: \", round(accuracy, 4))\n",
    "print(f\"Average time taken per sample in ms\", round((total_time/total_samples)*1000, 2))\n",
    "print(\"Preprocessing MisPredictions: \", preprocessingMisPredictions)\n",
    "# print (\"The following Predictions were False :\")\n",
    "\n",
    "print(modelpath)\n",
    "print (\"Speaker Identified Successfully\")\n",
    "with open(\"scalibility_test.txt\", \"w\") as f:\n",
    "    print(\"Study's dataset false predictions (True: False)\", file=f)\n",
    "    for prediction in false_predictions:\n",
    "        print(prediction, file=f)\n",
    "    print(\"Accuracy: \", round(accuracy, 4), file=f)\n",
    "    print(f\"Average time taken per sample in ms\", round((total_time/total_samples)*1000, 2), file=f, end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Models:  6\n",
      "Testing the model with all the samples...\n",
      "Sherif:Youssef, Sherif:Reem, Sherif:Abdelrahman, Sherif:Abdelrahman, Sherif:Reem, Sherif:Reem, Sherif:Reem, Sherif:Reem, Abdelrahman:Youssef, Abdelrahman:Omar, Abdelrahman:Reem, Omar:Sherif, Reem:Renad, Renad:Reem, Renad:Reem, Renad:Reem, 16 48.0\n",
      "Accuracy:  0.6667\n",
      "Average time taken per sample in ms 13.12\n",
      "Preprocessing MisPredictions:  0\n",
      "2-gmm_models_scale_test_dataset/\n",
      "Speaker Identified Successfully\n"
     ]
    }
   ],
   "source": [
    "test = \"normalized\"\n",
    "subset = \"testing_different\"\n",
    "\n",
    "# test = \"normalized\"\n",
    "# subset = \"testing_different\"\n",
    "\n",
    "source = f\"../../dataset/{test}_dataset/{subset}/\"\n",
    "\n",
    "#path where training speakers will be saved\n",
    "modelpath = dest\n",
    "\n",
    "gmm_files = [os.path.join(modelpath,fname) for fname in \n",
    "              os.listdir(modelpath) if fname.endswith('.gmm')]\n",
    "\n",
    "#Load the Gaussian Models\n",
    "models    = [cPickle.load(open(fname,'rb')) for fname in gmm_files]\n",
    "print(\"Loaded Models: \", len(models))\n",
    "speakers   = [fname.split(\"/\")[-1].split(\".gmm\")[0] for fname \n",
    "              in gmm_files]\n",
    "\n",
    "error = 0\n",
    "total_samples = 0.0\n",
    "\n",
    "external_class_true_count = 0\n",
    "# print(\"Press '1' for checking a single Audio or Press '0' for testing a complete set of audio with Accuracy?\")\n",
    "# take=int(input().strip())\n",
    "\n",
    "predictions = []\n",
    "gender_predictions = []\n",
    "gender_gt = []\n",
    "y_true = []\n",
    "preprocessingMisPredictions = 0\n",
    "print(\"Testing the model with all the samples...\")\n",
    "false_predictions = []\n",
    "test_file = \"Testing_audio_Path.txt\"       \n",
    "# file_paths = open(test_file,'r')\n",
    "file_paths = glob.glob(f\"{source}/**/*.wav\", recursive=True)\n",
    "\n",
    "# print(file_paths)\n",
    "\n",
    "# Read the test directory and get the list of test audio files \n",
    "avg_time = 0\n",
    "total_time = 0\n",
    "for path in file_paths:\n",
    "    time1 = time.time()   \n",
    "    total_samples+= 1.0\n",
    "    path=path.strip()\n",
    "    # print(\"Testing Audio : \", path)\n",
    "    sr,audio = read(path)\n",
    "    vector   = extract_features(audio,sr)\n",
    "    # print(vector.shape)\n",
    "    log_likelihood = np.zeros(len(models)) \n",
    "    for i in range(len(models)):\n",
    "        gmm    = models[i]  #checking with each model one by one\n",
    "        scores = np.array(gmm.score(vector))\n",
    "        log_likelihood[i] = scores.sum()\n",
    "    winner=np.argmax(log_likelihood)\n",
    "    winner_score = log_likelihood[winner]\n",
    "    predicted_speaker = speakers[winner]\n",
    "    speaker_label = path.split(\"/\")[-2]\n",
    "    # print(\"OK score: \", winner_score)\n",
    "    predictions.append(predicted_speaker)\n",
    "    y_true.append(speaker_label)\n",
    "\n",
    "    if predicted_speaker != speaker_label:\n",
    "        print(f\"{speaker_label}:{predicted_speaker}, \", end=\"\")\n",
    "        error += 1\n",
    "        false_predictions.append(f\"{speaker_label}:{predicted_speaker} ({path.split('/')[-1]})\")\n",
    "    # time.sleep(1.0)\n",
    "    sample_time = time.time() - time1\n",
    "    total_time += sample_time\n",
    "print (error, total_samples)\n",
    "accuracy = ((total_samples - error) / total_samples)\n",
    "\n",
    "print (\"Accuracy: \", round(accuracy, 4))\n",
    "print(f\"Average time taken per sample in ms\", round((total_time/total_samples)*1000, 2))\n",
    "print(\"Preprocessing MisPredictions: \", preprocessingMisPredictions)\n",
    "# print (\"The following Predictions were False :\")\n",
    "\n",
    "print(modelpath)\n",
    "print (\"Speaker Identified Successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VidTIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fdms0-011', 'fjem0-070', 'fdac1-021', 'mrgg0-050', 'mpdf0-060', 'fpkt0-084', 'fcft0-030', 'fcmh0-078', 'mbdg0-079', 'mjar0-031', 'fkms0-062', 'mmdm2-087', 'mcem0-044', 'mtmr0-066', 'mccs0-007', 'fdrd1-056', 'fedw0-082', 'fadg0-016', 'fgjd0-059', 'fjas0-074', 'msjs1-002', 'mjsw0-054', 'mrcz0-068', 'faks0-076', 'mdab0-023', 'fjwb0-080', 'mbjk0-020', 'mabw0-043', 'mpgl0-048', 'mgwt0-017', 'mtas1-026', 'fram1-035', 'fjre0-051', 'mdld0-018', 'fcrh0-027', 'mreb0-061', 'mrjo0-008', 'fcmr0-045', 'felc0-083', 'mmdb1-013', 'mwbt0-001', 'mstk0-064', 'mdbb0-041']\n",
      "fdms0-011/sa1.wav\n",
      "fdms0-011/sa2.wav\n",
      "fdms0-011/si1848.wav\n",
      "fdms0-011/sx138.wav\n",
      "fdms0-011/sx228.wav\n",
      "fdms0-011/sx318.wav\n",
      "fdms0-011/sx408.wav\n",
      "fdms0-011/sx48.wav\n",
      "fdms0-011.gmm\n",
      "+ modeling completed for speaker: fdms0-011.gmm  with data point =  (3184, 22)\n",
      "fjem0-070/sa1.wav\n",
      "fjem0-070/sa2.wav\n",
      "fjem0-070/si1264.wav\n",
      "fjem0-070/si1894.wav\n",
      "fjem0-070/sx184.wav\n",
      "fjem0-070/sx274.wav\n",
      "fjem0-070/sx364.wav\n",
      "fjem0-070/sx94.wav\n",
      "fjem0-070.gmm\n",
      "+ modeling completed for speaker: fjem0-070.gmm  with data point =  (4488, 22)\n",
      "fdac1-021/sa1.wav\n",
      "fdac1-021/si1474.wav\n",
      "fdac1-021/si2104.wav\n",
      "fdac1-021/si844.wav\n",
      "fdac1-021/sx124.wav\n",
      "fdac1-021/sx304.wav\n",
      "fdac1-021/sx34.wav\n",
      "fdac1-021/sx394.wav\n",
      "fdac1-021.gmm\n",
      "+ modeling completed for speaker: fdac1-021.gmm  with data point =  (2972, 22)\n",
      "mrgg0-050/sa1.wav\n",
      "mrgg0-050/sa2.wav\n",
      "mrgg0-050/si1199.wav\n",
      "mrgg0-050/si1829.wav\n",
      "mrgg0-050/si569.wav\n",
      "mrgg0-050/sx119.wav\n",
      "mrgg0-050/sx299.wav\n",
      "mrgg0-050/sx389.wav\n",
      "mrgg0-050.gmm\n",
      "+ modeling completed for speaker: mrgg0-050.gmm  with data point =  (4380, 22)\n",
      "mpdf0-060/sa1.wav\n",
      "mpdf0-060/sa2.wav\n",
      "mpdf0-060/si1542.wav\n",
      "mpdf0-060/sx102.wav\n",
      "mpdf0-060/sx12.wav\n",
      "mpdf0-060/sx192.wav\n",
      "mpdf0-060/sx282.wav\n",
      "mpdf0-060/sx372.wav\n",
      "mpdf0-060.gmm\n",
      "+ modeling completed for speaker: mpdf0-060.gmm  with data point =  (3160, 22)\n",
      "fpkt0-084/sa1.wav\n",
      "fpkt0-084/sa2.wav\n",
      "fpkt0-084/si1538.wav\n",
      "fpkt0-084/si2168.wav\n",
      "fpkt0-084/si908.wav\n",
      "fpkt0-084/sx188.wav\n",
      "fpkt0-084/sx368.wav\n",
      "fpkt0-084/sx98.wav\n",
      "fpkt0-084.gmm\n",
      "+ modeling completed for speaker: fpkt0-084.gmm  with data point =  (3328, 22)\n",
      "fcft0-030/sa1.wav\n",
      "fcft0-030/si1178.wav\n",
      "fcft0-030/si1808.wav\n",
      "fcft0-030/sx188.wav\n",
      "fcft0-030/sx278.wav\n",
      "fcft0-030/sx368.wav\n",
      "fcft0-030/sx8.wav\n",
      "fcft0-030/sx98.wav\n",
      "fcft0-030.gmm\n",
      "+ modeling completed for speaker: fcft0-030.gmm  with data point =  (3524, 22)\n",
      "fcmh0-078/sa1.wav\n",
      "fcmh0-078/sa2.wav\n",
      "fcmh0-078/si1454.wav\n",
      "fcmh0-078/si2084.wav\n",
      "fcmh0-078/si824.wav\n",
      "fcmh0-078/sx104.wav\n",
      "fcmh0-078/sx194.wav\n",
      "fcmh0-078/sx284.wav\n",
      "fcmh0-078.gmm\n",
      "+ modeling completed for speaker: fcmh0-078.gmm  with data point =  (3204, 22)\n",
      "mbdg0-079/sa1.wav\n",
      "mbdg0-079/si2093.wav\n",
      "mbdg0-079/si833.wav\n",
      "mbdg0-079/sx113.wav\n",
      "mbdg0-079/sx203.wav\n",
      "mbdg0-079/sx23.wav\n",
      "mbdg0-079/sx293.wav\n",
      "mbdg0-079/sx383.wav\n",
      "mbdg0-079.gmm\n",
      "+ modeling completed for speaker: mbdg0-079.gmm  with data point =  (3128, 22)\n",
      "mjar0-031/sa1.wav\n",
      "mjar0-031/sa2.wav\n",
      "mjar0-031/si1988.wav\n",
      "mjar0-031/si2247.wav\n",
      "mjar0-031/si728.wav\n",
      "mjar0-031/sx188.wav\n",
      "mjar0-031/sx368.wav\n",
      "mjar0-031/sx98.wav\n",
      "mjar0-031.gmm\n",
      "+ modeling completed for speaker: mjar0-031.gmm  with data point =  (3524, 22)\n",
      "fkms0-062/sa1.wav\n",
      "fkms0-062/si1490.wav\n",
      "fkms0-062/si2120.wav\n",
      "fkms0-062/si860.wav\n",
      "fkms0-062/sx140.wav\n",
      "fkms0-062/sx320.wav\n",
      "fkms0-062/sx410.wav\n",
      "fkms0-062/sx50.wav\n",
      "fkms0-062.gmm\n",
      "+ modeling completed for speaker: fkms0-062.gmm  with data point =  (3344, 22)\n",
      "mmdm2-087/sa1.wav\n",
      "mmdm2-087/sa2.wav\n",
      "mmdm2-087/si1555.wav\n",
      "mmdm2-087/sx102.wav\n",
      "mmdm2-087/sx12.wav\n",
      "mmdm2-087/sx192.wav\n",
      "mmdm2-087/sx282.wav\n",
      "mmdm2-087/sx372.wav\n",
      "mmdm2-087.gmm\n",
      "+ modeling completed for speaker: mmdm2-087.gmm  with data point =  (2968, 22)\n",
      "mcem0-044/sa1.wav\n",
      "mcem0-044/sa2.wav\n",
      "mcem0-044/si768.wav\n",
      "mcem0-044/sx138.wav\n",
      "mcem0-044/sx228.wav\n",
      "mcem0-044/sx318.wav\n",
      "mcem0-044/sx408.wav\n",
      "mcem0-044/sx48.wav\n",
      "mcem0-044.gmm\n",
      "+ modeling completed for speaker: mcem0-044.gmm  with data point =  (2900, 22)\n",
      "mtmr0-066/sa1.wav\n",
      "mtmr0-066/si1303.wav\n",
      "mtmr0-066/si673.wav\n",
      "mtmr0-066/sx133.wav\n",
      "mtmr0-066/sx223.wav\n",
      "mtmr0-066/sx313.wav\n",
      "mtmr0-066/sx403.wav\n",
      "mtmr0-066/sx43.wav\n",
      "mtmr0-066.gmm\n",
      "+ modeling completed for speaker: mtmr0-066.gmm  with data point =  (3424, 22)\n",
      "mccs0-007/sa1.wav\n",
      "mccs0-007/sa2.wav\n",
      "mccs0-007/si1469.wav\n",
      "mccs0-007/si2099.wav\n",
      "mccs0-007/si839.wav\n",
      "mccs0-007/sx119.wav\n",
      "mccs0-007/sx209.wav\n",
      "mccs0-007/sx299.wav\n",
      "mccs0-007.gmm\n",
      "+ modeling completed for speaker: mccs0-007.gmm  with data point =  (2880, 22)\n",
      "fdrd1-056/sa1.wav\n",
      "fdrd1-056/sa2.wav\n",
      "fdrd1-056/si1544.wav\n",
      "fdrd1-056/si2149.wav\n",
      "fdrd1-056/sx104.wav\n",
      "fdrd1-056/sx194.wav\n",
      "fdrd1-056/sx284.wav\n",
      "fdrd1-056/sx374.wav\n",
      "fdrd1-056.gmm\n",
      "+ modeling completed for speaker: fdrd1-056.gmm  with data point =  (3416, 22)\n",
      "fedw0-082/sa1.wav\n",
      "fedw0-082/si1084.wav\n",
      "fedw0-082/si1653.wav\n",
      "fedw0-082/si1714.wav\n",
      "fedw0-082/sx184.wav\n",
      "fedw0-082/sx274.wav\n",
      "fedw0-082/sx4.wav\n",
      "fedw0-082/sx94.wav\n",
      "fedw0-082.gmm\n",
      "+ modeling completed for speaker: fedw0-082.gmm  with data point =  (2708, 22)\n",
      "fadg0-016/sa1.wav\n",
      "fadg0-016/sa2.wav\n",
      "fadg0-016/si1279.wav\n",
      "fadg0-016/si1909.wav\n",
      "fadg0-016/si649.wav\n",
      "fadg0-016/sx19.wav\n",
      "fadg0-016/sx199.wav\n",
      "fadg0-016/sx289.wav\n",
      "fadg0-016.gmm\n",
      "+ modeling completed for speaker: fadg0-016.gmm  with data point =  (3968, 22)\n",
      "fgjd0-059/sa1.wav\n",
      "fgjd0-059/sa2.wav\n",
      "fgjd0-059/si1179.wav\n",
      "fgjd0-059/si549.wav\n",
      "fgjd0-059/si818.wav\n",
      "fgjd0-059/sx279.wav\n",
      "fgjd0-059/sx369.wav\n",
      "fgjd0-059/sx9.wav\n",
      "fgjd0-059.gmm\n",
      "+ modeling completed for speaker: fgjd0-059.gmm  with data point =  (3404, 22)\n",
      "fjas0-074/sa1.wav\n",
      "fjas0-074/si1400.wav\n",
      "fjas0-074/si2030.wav\n",
      "fjas0-074/si770.wav\n",
      "fjas0-074/sx140.wav\n",
      "fjas0-074/sx320.wav\n",
      "fjas0-074/sx410.wav\n",
      "fjas0-074/sx50.wav\n",
      "fjas0-074.gmm\n",
      "+ modeling completed for speaker: fjas0-074.gmm  with data point =  (3532, 22)\n",
      "msjs1-002/sa1.wav\n",
      "msjs1-002/sa2.wav\n",
      "msjs1-002/si1899.wav\n",
      "msjs1-002/si639.wav\n",
      "msjs1-002/si869.wav\n",
      "msjs1-002/sx279.wav\n",
      "msjs1-002/sx369.wav\n",
      "msjs1-002/sx9.wav\n",
      "msjs1-002.gmm\n",
      "+ modeling completed for speaker: msjs1-002.gmm  with data point =  (3752, 22)\n",
      "mjsw0-054/sa1.wav\n",
      "mjsw0-054/sa2.wav\n",
      "mjsw0-054/si1010.wav\n",
      "mjsw0-054/si2270.wav\n",
      "mjsw0-054/sx110.wav\n",
      "mjsw0-054/sx20.wav\n",
      "mjsw0-054/sx200.wav\n",
      "mjsw0-054/sx380.wav\n",
      "mjsw0-054.gmm\n",
      "+ modeling completed for speaker: mjsw0-054.gmm  with data point =  (2880, 22)\n",
      "mrcz0-068/sa1.wav\n",
      "mrcz0-068/sa2.wav\n",
      "mrcz0-068/si1541.wav\n",
      "mrcz0-068/si911.wav\n",
      "mrcz0-068/sx11.wav\n",
      "mrcz0-068/sx191.wav\n",
      "mrcz0-068/sx281.wav\n",
      "mrcz0-068/sx371.wav\n",
      "mrcz0-068.gmm\n",
      "+ modeling completed for speaker: mrcz0-068.gmm  with data point =  (3360, 22)\n",
      "faks0-076/sa1.wav\n",
      "faks0-076/si1573.wav\n",
      "faks0-076/si943.wav\n",
      "faks0-076/sx133.wav\n",
      "faks0-076/sx223.wav\n",
      "faks0-076/sx313.wav\n",
      "faks0-076/sx403.wav\n",
      "faks0-076/sx43.wav\n",
      "faks0-076.gmm\n",
      "+ modeling completed for speaker: faks0-076.gmm  with data point =  (3156, 22)\n",
      "mdab0-023/sa1.wav\n",
      "mdab0-023/sa2.wav\n",
      "mdab0-023/si1039.wav\n",
      "mdab0-023/si1669.wav\n",
      "mdab0-023/sx139.wav\n",
      "mdab0-023/sx229.wav\n",
      "mdab0-023/sx319.wav\n",
      "mdab0-023/sx49.wav\n",
      "mdab0-023.gmm\n",
      "+ modeling completed for speaker: mdab0-023.gmm  with data point =  (3692, 22)\n",
      "fjwb0-080/sa1.wav\n",
      "fjwb0-080/si1265.wav\n",
      "fjwb0-080/si635.wav\n",
      "fjwb0-080/si992.wav\n",
      "fjwb0-080/sx185.wav\n",
      "fjwb0-080/sx365.wav\n",
      "fjwb0-080/sx5.wav\n",
      "fjwb0-080/sx95.wav\n",
      "fjwb0-080.gmm\n",
      "+ modeling completed for speaker: fjwb0-080.gmm  with data point =  (3968, 22)\n",
      "mbjk0-020/sa1.wav\n",
      "mbjk0-020/si2128.wav\n",
      "mbjk0-020/si545.wav\n",
      "mbjk0-020/sx185.wav\n",
      "mbjk0-020/sx275.wav\n",
      "mbjk0-020/sx365.wav\n",
      "mbjk0-020/sx5.wav\n",
      "mbjk0-020/sx95.wav\n",
      "mbjk0-020.gmm\n",
      "+ modeling completed for speaker: mbjk0-020.gmm  with data point =  (3384, 22)\n",
      "mabw0-043/sa2.wav\n",
      "mabw0-043/si1230.wav\n",
      "mabw0-043/si1664.wav\n",
      "mabw0-043/si2294.wav\n",
      "mabw0-043/sx134.wav\n",
      "mabw0-043/sx314.wav\n",
      "mabw0-043/sx404.wav\n",
      "mabw0-043/sx44.wav\n",
      "mabw0-043.gmm\n",
      "+ modeling completed for speaker: mabw0-043.gmm  with data point =  (4104, 22)\n",
      "mpgl0-048/sa1.wav\n",
      "mpgl0-048/si1729.wav\n",
      "mpgl0-048/si469.wav\n",
      "mpgl0-048/sx109.wav\n",
      "mpgl0-048/sx19.wav\n",
      "mpgl0-048/sx199.wav\n",
      "mpgl0-048/sx289.wav\n",
      "mpgl0-048/sx379.wav\n",
      "mpgl0-048.gmm\n",
      "+ modeling completed for speaker: mpgl0-048.gmm  with data point =  (3112, 22)\n",
      "mgwt0-017/sa1.wav\n",
      "mgwt0-017/sa2.wav\n",
      "mgwt0-017/si1539.wav\n",
      "mgwt0-017/si2169.wav\n",
      "mgwt0-017/si909.wav\n",
      "mgwt0-017/sx369.wav\n",
      "mgwt0-017/sx9.wav\n",
      "mgwt0-017/sx99.wav\n",
      "mgwt0-017.gmm\n",
      "+ modeling completed for speaker: mgwt0-017.gmm  with data point =  (3460, 22)\n",
      "mtas1-026/sa1.wav\n",
      "mtas1-026/sa2.wav\n",
      "mtas1-026/si2098.wav\n",
      "mtas1-026/si838.wav\n",
      "mtas1-026/sx208.wav\n",
      "mtas1-026/sx28.wav\n",
      "mtas1-026/sx298.wav\n",
      "mtas1-026/sx388.wav\n",
      "mtas1-026.gmm\n",
      "+ modeling completed for speaker: mtas1-026.gmm  with data point =  (3172, 22)\n",
      "fram1-035/sa1.wav\n",
      "fram1-035/si1360.wav\n",
      "fram1-035/si730.wav\n",
      "fram1-035/sx10.wav\n",
      "fram1-035/sx100.wav\n",
      "fram1-035/sx190.wav\n",
      "fram1-035/sx280.wav\n",
      "fram1-035/sx370.wav\n",
      "fram1-035.gmm\n",
      "+ modeling completed for speaker: fram1-035.gmm  with data point =  (3628, 22)\n",
      "fjre0-051/sa1.wav\n",
      "fjre0-051/si1116.wav\n",
      "fjre0-051/si1587.wav\n",
      "fjre0-051/si1746.wav\n",
      "fjre0-051/sx126.wav\n",
      "fjre0-051/sx216.wav\n",
      "fjre0-051/sx306.wav\n",
      "fjre0-051/sx396.wav\n",
      "fjre0-051.gmm\n",
      "+ modeling completed for speaker: fjre0-051.gmm  with data point =  (3264, 22)\n",
      "mdld0-018/sa1.wav\n",
      "mdld0-018/si1543.wav\n",
      "mdld0-018/si2173.wav\n",
      "mdld0-018/si913.wav\n",
      "mdld0-018/sx13.wav\n",
      "mdld0-018/sx193.wav\n",
      "mdld0-018/sx283.wav\n",
      "mdld0-018/sx373.wav\n",
      "mdld0-018.gmm\n",
      "+ modeling completed for speaker: mdld0-018.gmm  with data point =  (3536, 22)\n",
      "fcrh0-027/sa1.wav\n",
      "fcrh0-027/sa2.wav\n",
      "fcrh0-027/si1088.wav\n",
      "fcrh0-027/si1718.wav\n",
      "fcrh0-027/sx188.wav\n",
      "fcrh0-027/sx278.wav\n",
      "fcrh0-027/sx368.wav\n",
      "fcrh0-027/sx98.wav\n",
      "fcrh0-027.gmm\n",
      "+ modeling completed for speaker: fcrh0-027.gmm  with data point =  (3532, 22)\n",
      "mreb0-061/sa1.wav\n",
      "mreb0-061/sa2.wav\n",
      "mreb0-061/si2005.wav\n",
      "mreb0-061/si745.wav\n",
      "mreb0-061/sx115.wav\n",
      "mreb0-061/sx205.wav\n",
      "mreb0-061/sx295.wav\n",
      "mreb0-061/sx385.wav\n",
      "mreb0-061.gmm\n",
      "+ modeling completed for speaker: mreb0-061.gmm  with data point =  (3808, 22)\n",
      "mrjo0-008/sa1.wav\n",
      "mrjo0-008/sa2.wav\n",
      "mrjo0-008/si1364.wav\n",
      "mrjo0-008/si734.wav\n",
      "mrjo0-008/sx104.wav\n",
      "mrjo0-008/sx194.wav\n",
      "mrjo0-008/sx284.wav\n",
      "mrjo0-008/sx374.wav\n",
      "mrjo0-008.gmm\n",
      "+ modeling completed for speaker: mrjo0-008.gmm  with data point =  (3440, 22)\n",
      "fcmr0-045/sa1.wav\n",
      "fcmr0-045/sa2.wav\n",
      "fcmr0-045/si1105.wav\n",
      "fcmr0-045/sx115.wav\n",
      "fcmr0-045/sx205.wav\n",
      "fcmr0-045/sx25.wav\n",
      "fcmr0-045/sx295.wav\n",
      "fcmr0-045/sx385.wav\n",
      "fcmr0-045.gmm\n",
      "+ modeling completed for speaker: fcmr0-045.gmm  with data point =  (3204, 22)\n",
      "felc0-083/sa1.wav\n",
      "felc0-083/si1386.wav\n",
      "felc0-083/si2016.wav\n",
      "felc0-083/si756.wav\n",
      "felc0-083/sx126.wav\n",
      "felc0-083/sx216.wav\n",
      "felc0-083/sx306.wav\n",
      "felc0-083/sx36.wav\n",
      "felc0-083.gmm\n",
      "+ modeling completed for speaker: felc0-083.gmm  with data point =  (3596, 22)\n",
      "mmdb1-013/sa1.wav\n",
      "mmdb1-013/sa2.wav\n",
      "mmdb1-013/si995.wav\n",
      "mmdb1-013/sx1625.wav\n",
      "mmdb1-013/sx275.wav\n",
      "mmdb1-013/sx365.wav\n",
      "mmdb1-013/sx5.wav\n",
      "mmdb1-013/sx95.wav\n",
      "mmdb1-013.gmm\n",
      "+ modeling completed for speaker: mmdb1-013.gmm  with data point =  (3324, 22)\n",
      "mwbt0-001/sa1.wav\n",
      "mwbt0-001/si1553.wav\n",
      "mwbt0-001/si923.wav\n",
      "mwbt0-001/sx113.wav\n",
      "mwbt0-001/sx203.wav\n",
      "mwbt0-001/sx23.wav\n",
      "mwbt0-001/sx293.wav\n",
      "mwbt0-001/sx383.wav\n",
      "mwbt0-001.gmm\n",
      "+ modeling completed for speaker: mwbt0-001.gmm  with data point =  (3388, 22)\n",
      "mstk0-064/sa1.wav\n",
      "mstk0-064/sa2.wav\n",
      "mstk0-064/si1024.wav\n",
      "mstk0-064/sx124.wav\n",
      "mstk0-064/sx214.wav\n",
      "mstk0-064/sx304.wav\n",
      "mstk0-064/sx34.wav\n",
      "mstk0-064/sx394.wav\n",
      "mstk0-064.gmm\n",
      "+ modeling completed for speaker: mstk0-064.gmm  with data point =  (2904, 22)\n",
      "mdbb0-041/sa1.wav\n",
      "mdbb0-041/sa2.wav\n",
      "mdbb0-041/si565.wav\n",
      "mdbb0-041/sx115.wav\n",
      "mdbb0-041/sx1195.wav\n",
      "mdbb0-041/sx205.wav\n",
      "mdbb0-041/sx295.wav\n",
      "mdbb0-041/sx385.wav\n",
      "mdbb0-041.gmm\n",
      "+ modeling completed for speaker: mdbb0-041.gmm  with data point =  (3332, 22)\n",
      "Total Time taken:  24.8 seconds\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "training = True\n",
    "dest = \"2-gmm_models_scale_test_VidTIMIT/\"\n",
    "\n",
    "training_size = 8\n",
    "\n",
    "test = \"normalized\"\n",
    "\n",
    "subset = \"training\"\n",
    "audios = f\"../../dataset/{test}_dataset/VidTIMIT_{subset}/\"\n",
    "\n",
    "directories = os.listdir(audios)\n",
    "print(directories)\n",
    "\n",
    "\n",
    "\n",
    "for directory in directories:\n",
    "    # 1 speaker\n",
    "    # directory = directories[0]\n",
    "\n",
    "    # List all files in the directory\n",
    "    file_paths = os.listdir(audios + directory)\n",
    "    file_paths = sorted(file_paths)\n",
    "    file_paths = file_paths[:training_size]\n",
    "    # print(file_paths)\n",
    "    # Extracting features for each speaker\n",
    "    features = np.asarray(())\n",
    "    for path in file_paths:\n",
    "        path = directory + \"/\" + path    \n",
    "        path = path.strip()   \n",
    "        print (path)\n",
    "        \n",
    "        # read the audio\n",
    "        sr,audio = read(audios+path)\n",
    "        \n",
    "        # extract 40 dimensional MFCC & delta MFCC features\n",
    "        vector   = extract_features(audio,sr)\n",
    "        \n",
    "        if features.size == 0:\n",
    "            features = vector\n",
    "        else:\n",
    "            features = np.vstack((features, vector))\n",
    "        # print(features.shape)\n",
    "\n",
    "\n",
    "    if training and subset == \"training\":            \n",
    "        gmm = GMM(n_components = 5, covariance_type='diag',n_init = 3)\n",
    "        gmm.fit(features)\n",
    "        # dumping the trained gaussian model\n",
    "        picklefile = directory + \".gmm\"\n",
    "        print(picklefile)\n",
    "        cPickle.dump(gmm,open(dest + picklefile,'wb'))\n",
    "        print ('+ modeling completed for speaker:',picklefile,\" with data point = \",features.shape)   \n",
    "        features = np.asarray(())\n",
    "print(\"Total Time taken: \", round(time.time() - time1, 2), \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Models:  43\n",
      "Testing the model with all the samples...\n",
      "fjem0-070:mpdf0-060, fdac1-021:fdms0-011, fpkt0-084:fadg0-016, fcft0-030:fdms0-011, fcmh0-078:fpkt0-084, fcmh0-078:fdac1-021, fkms0-062:mrcz0-068, mtmr0-066:mdab0-023, mccs0-007:mreb0-061, mdab0-023:mwbt0-001, fjwb0-080:fadg0-016, mabw0-043:mdbb0-041, mpgl0-048:mrcz0-068, mtas1-026:msjs1-002, fcrh0-027:fcmh0-078, mreb0-061:mpdf0-060, mdbb0-041:mtmr0-066, 17 86.0\n",
      "Accuracy:  0.8023\n",
      "Average time taken per sample in ms 30.19\n",
      "Preprocessing MisPredictions:  0\n",
      "2-gmm_models_scale_test_VidTIMIT/\n",
      "Speaker Identified Successfully\n"
     ]
    }
   ],
   "source": [
    "test = \"normalized\"\n",
    "subset = \"VidTIMIT_testing\"\n",
    "\n",
    "# test = \"normalized\"\n",
    "# subset = \"testing_different\"\n",
    "\n",
    "source = f\"../../dataset/{test}_dataset/{subset}/\"\n",
    "\n",
    "#path where training speakers will be saved\n",
    "modelpath = dest\n",
    "\n",
    "gmm_files = [os.path.join(modelpath,fname) for fname in \n",
    "              os.listdir(modelpath) if fname.endswith('.gmm')]\n",
    "\n",
    "#Load the Gaussian Models\n",
    "models    = [cPickle.load(open(fname,'rb')) for fname in gmm_files]\n",
    "print(\"Loaded Models: \", len(models))\n",
    "speakers   = [fname.split(\"/\")[-1].split(\".gmm\")[0] for fname \n",
    "              in gmm_files]\n",
    "\n",
    "error = 0\n",
    "total_samples = 0.0\n",
    "\n",
    "external_class_true_count = 0\n",
    "# print(\"Press '1' for checking a single Audio or Press '0' for testing a complete set of audio with Accuracy?\")\n",
    "# take=int(input().strip())\n",
    "\n",
    "predictions = []\n",
    "gender_predictions = []\n",
    "gender_gt = []\n",
    "y_true = []\n",
    "preprocessingMisPredictions = 0\n",
    "print(\"Testing the model with all the samples...\")\n",
    "false_predictions = []\n",
    "test_file = \"Testing_audio_Path.txt\"       \n",
    "# file_paths = open(test_file,'r')\n",
    "file_paths = glob.glob(f\"{source}/**/*.wav\", recursive=True)\n",
    "\n",
    "# Read the test directory and get the list of test audio files \n",
    "avg_time = 0\n",
    "total_time = 0\n",
    "for path in file_paths:\n",
    "    time1 = time.time()   \n",
    "    total_samples+= 1.0\n",
    "    path=path.strip()\n",
    "    # print(\"Testing Audio : \", path)\n",
    "    sr,audio = read(path)\n",
    "    vector   = extract_features(audio,sr)\n",
    "    # print(vector.shape)\n",
    "    log_likelihood = np.zeros(len(models)) \n",
    "    for i in range(len(models)):\n",
    "        gmm    = models[i]  #checking with each model one by one\n",
    "        scores = np.array(gmm.score(vector))\n",
    "        log_likelihood[i] = scores.sum()\n",
    "    winner=np.argmax(log_likelihood)\n",
    "    winner_score = log_likelihood[winner]\n",
    "    predicted_speaker = speakers[winner]\n",
    "    speaker_label = path.split(\"/\")[-2]\n",
    "    # print(\"OK score: \", winner_score)\n",
    "    predictions.append(predicted_speaker)\n",
    "    y_true.append(speaker_label)\n",
    "\n",
    "    if predicted_speaker != speaker_label:\n",
    "        print(f\"{speaker_label}:{predicted_speaker}, \", end=\"\")\n",
    "        error += 1\n",
    "        false_predictions.append(f\"{speaker_label}:{predicted_speaker} ({path.split('/')[-1]})\")\n",
    "    # time.sleep(1.0)\n",
    "    sample_time = time.time() - time1\n",
    "    total_time += sample_time\n",
    "print (error, total_samples)\n",
    "accuracy = ((total_samples - error) / total_samples)\n",
    "\n",
    "print (\"Accuracy: \", round(accuracy, 4))\n",
    "print(f\"Average time taken per sample in ms\", round((total_time/total_samples)*1000, 2))\n",
    "print(\"Preprocessing MisPredictions: \", preprocessingMisPredictions)\n",
    "# print (\"The following Predictions were False :\")\n",
    "\n",
    "print(modelpath)\n",
    "print (\"Speaker Identified Successfully\")\n",
    "with open(\"scalibility_test.txt\", \"a\") as f:\n",
    "    print(\"VidTIMIT false predictions (True: False)\", file=f)\n",
    "    for prediction in sorted(false_predictions):\n",
    "        print(prediction, file=f)\n",
    "    print(\"Accuracy: \", round(accuracy, 4), file=f)\n",
    "    print(f\"Average time taken per sample in ms\", round((total_time/total_samples)*1000, 2), file=f, end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "directory1 = \"2-gmm_models_scale_test_dataset\"\n",
    "directory2 = \"2-gmm_models_scale_test_VidTIMIT\"\n",
    "directory_dest = \"2-gmm_models_scale_test_combined\"\n",
    "\n",
    "# Copy files from directory1 to directory_dest\n",
    "for filename in os.listdir(directory1):\n",
    "    src = os.path.join(directory1, filename)\n",
    "    dst = os.path.join(directory_dest, filename)\n",
    "    shutil.copy(src, dst)\n",
    "\n",
    "# Copy files from directory2 to directory_dest\n",
    "for filename in os.listdir(directory2):\n",
    "    src = os.path.join(directory2, filename)\n",
    "    dst = os.path.join(directory_dest, filename)\n",
    "    shutil.copy(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Models:  49\n",
      "Testing the model with all the samples...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renad:Reem, Renad:Reem, fjem0-070:mpdf0-060, fdac1-021:fdms0-011, fpkt0-084:fadg0-016, fcft0-030:fdms0-011, fcmh0-078:fpkt0-084, fcmh0-078:fdac1-021, fkms0-062:mrcz0-068, mtmr0-066:mdab0-023, mccs0-007:mreb0-061, mdab0-023:mwbt0-001, fjwb0-080:fadg0-016, mabw0-043:mdbb0-041, mpgl0-048:mrcz0-068, mtas1-026:msjs1-002, fcrh0-027:fcmh0-078, mreb0-061:mpdf0-060, mdbb0-041:mtmr0-066, 19 206.0\n",
      "Accuracy:  0.9078\n",
      "Average time taken per sample in ms 29.21\n",
      "Preprocessing MisPredictions:  0\n",
      "2-gmm_models_scale_test_combined\n",
      "Speaker Identified Successfully\n"
     ]
    }
   ],
   "source": [
    "test = \"normalized\"\n",
    "subset = \"testing_same\"\n",
    "\n",
    "# test = \"normalized\"\n",
    "# subset = \"testing_different\"\n",
    "\n",
    "source = f\"../../dataset/{test}_dataset/{subset}/\"\n",
    "source2 = f\"../../dataset/{test}_dataset/VidTIMIT_testing/\"\n",
    "\n",
    "#path where training speakers will be saved\n",
    "modelpath = directory_dest\n",
    "\n",
    "gmm_files = [os.path.join(modelpath,fname) for fname in \n",
    "              os.listdir(modelpath) if fname.endswith('.gmm')]\n",
    "\n",
    "#Load the Gaussian Models\n",
    "models    = [cPickle.load(open(fname,'rb')) for fname in gmm_files]\n",
    "print(\"Loaded Models: \", len(models))\n",
    "speakers   = [fname.split(\"/\")[-1].split(\".gmm\")[0] for fname \n",
    "              in gmm_files]\n",
    "\n",
    "error = 0\n",
    "total_samples = 0.0\n",
    "\n",
    "external_class_true_count = 0\n",
    "# print(\"Press '1' for checking a single Audio or Press '0' for testing a complete set of audio with Accuracy?\")\n",
    "# take=int(input().strip())\n",
    "\n",
    "predictions = []\n",
    "gender_predictions = []\n",
    "gender_gt = []\n",
    "y_true = []\n",
    "preprocessingMisPredictions = 0\n",
    "print(\"Testing the model with all the samples...\")\n",
    "false_predictions = []\n",
    "test_file = \"Testing_audio_Path.txt\"       \n",
    "# file_paths = open(test_file,'r')\n",
    "file_paths = glob.glob(f\"{source}/**/*.wav\", recursive=True)\n",
    "file_paths2 = glob.glob(f\"{source2}/**/*.wav\", recursive=True)\n",
    "\n",
    "file_paths = file_paths + file_paths2\n",
    "\n",
    "# Read the test directory and get the list of test audio files \n",
    "avg_time = 0\n",
    "total_time = 0\n",
    "for path in file_paths:\n",
    "    time1 = time.time()   \n",
    "    total_samples+= 1.0\n",
    "    path=path.strip()\n",
    "    # print(\"Testing Audio : \", path)\n",
    "    sr,audio = read(path)\n",
    "    vector   = extract_features(audio,sr)\n",
    "    # print(vector.shape)\n",
    "    log_likelihood = np.zeros(len(models)) \n",
    "    for i in range(len(models)):\n",
    "        gmm    = models[i]  #checking with each model one by one\n",
    "        scores = np.array(gmm.score(vector))\n",
    "        log_likelihood[i] = scores.sum()\n",
    "    winner=np.argmax(log_likelihood)\n",
    "    winner_score = log_likelihood[winner]\n",
    "    predicted_speaker = speakers[winner]\n",
    "    speaker_label = path.split(\"/\")[-2]\n",
    "    # print(\"OK score: \", winner_score)\n",
    "    predictions.append(predicted_speaker)\n",
    "    y_true.append(speaker_label)\n",
    "\n",
    "    if predicted_speaker != speaker_label:\n",
    "        print(f\"{speaker_label}:{predicted_speaker}, \", end=\"\")\n",
    "        error += 1\n",
    "        false_predictions.append(f\"{speaker_label}:{predicted_speaker} ({path.split('/')[-1]})\")\n",
    "\n",
    "    # time.sleep(1.0)\n",
    "    sample_time = time.time() - time1\n",
    "    total_time += sample_time\n",
    "print (error, total_samples)\n",
    "accuracy = ((total_samples - error) / total_samples)\n",
    "\n",
    "print (\"Accuracy: \", round(accuracy, 4))\n",
    "print(f\"Average time taken per sample in ms\", round((total_time/total_samples)*1000, 2))\n",
    "print(\"Preprocessing MisPredictions: \", preprocessingMisPredictions)\n",
    "# print (\"The following Predictions were False :\")\n",
    "\n",
    "print(modelpath)\n",
    "print (\"Speaker Identified Successfully\")\n",
    "with open(\"scalibility_test.txt\", \"a\") as f:\n",
    "    print(\"Combined dataset false predictions (True: False)\", file=f)\n",
    "    for prediction in false_predictions:\n",
    "        print(prediction, file=f)\n",
    "    print(\"Accuracy: \", round(accuracy, 4), file=f)\n",
    "    print(f\"Average time taken per sample in ms\", round((total_time/total_samples)*1000, 2), file=f, end=\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
